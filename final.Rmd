---
title: "final_recovery_time"
output: html_document
date: "2023-05-10"
---

```{r setup, include=FALSE}
library(ggplot2)
library(glmnet)
library(MASS)
library(tidyverse)
library(cowplot)
library(corrgram)
library(dplyr)
library(caret)
library(reshape2)
library(plyr)
library(corrplot)
library(rpart)
library(rpart.plot)
library(readr)
library(ranger)
```

Data Cleaning
```{r}
dat0 = read_csv(file = "final_used_data.csv")
#Draw a random sample of 2000 participants
set.seed(5220)
dat_new <- dat0[sample(1:10000, 2000),]
dat_new_01 <- na.omit(dat_new)

#Create a new data frame without id variable
dat <- dat0[ , !names(dat0) %in% c("id")]
attach(dat)
```

Data Preprocessing
```{r}
#Visualize response variable
par(mfrow = c(1, 2))
boxplot(dat$recovery_time, main = "COVID-19 Recovery Time")
hist(dat$recovery_time, main = "Distribution of Rescovery Time", col = "lightblue",
                        xlab = "Recovery Time", prob = TRUE, ylim = c(0,0.03))
lines(density(dat$recovery_time))
abline(v = mean(dat$recovery_time), lty = "dashed", col = "red")
# The above plots show that the response variable is right-skewed, so log-transformation was performed. 

dat$log_recovery_time <- log(dat$recovery_time)
par(mfrow = c(1, 2))
boxplot(dat$log_recovery_time, main = "COVID-19 Recovery Time")
hist(dat$log_recovery_time, main = "Distribution of Rescovery Time", col = "lightblue",
xlab = "Log Recovery Time", prob = TRUE, ylim = c(0,1))
lines(density(dat$log_recovery_time))
abline(v = mean(dat$log_recovery_time), lty = "dashed", col = "red")
#After the transformation, the outcome variable is normally distributed.
```

Data Partition
```{r split}
set.seed(5220)
trainRows <- createDataPartition(y = dat$log_recovery_time, p = 0.8, list = FALSE)

# Training data
dat_train = dat[trainRows, ]
x_train = model.matrix(log_recovery_time~.,dat)[trainRows, -1]
y_train = dat$log_recovery_time[trainRows]
# Test data
dat_test = dat[-trainRows, ]
x_test = model.matrix(log_recovery_time~.,dat)[-trainRows, -1]
y_test = dat$log_recovery_time[-trainRows]

```

Exploratory Analysis & Data Visualization
```{r}
# Summary statistics for each variable
summary(dat_train)

#Relocate columns putting non-discrete predictors together
dat1 =
  dat %>% 
  select(age,bmi,sbp,ldl,log_recovery_time)

# Correlation Plot
dat2 <- model.matrix(log_recovery_time ~ ., dat1)[ ,-1]
x <- dat2[trainRows,]
corrplot(cor(x))



# Convert non-numeric columns to numeric
dat_train1 <- dat_train
non_numeric_cols <- sapply(dat_train1, function(x) !is.numeric(x))
dat_train1[, non_numeric_cols] <- lapply(dat_train1[, non_numeric_cols], as.numeric)
view(dat_train1)


# Scatter Plot
theme1 <- trellis.par.get()
theme1$plot.symbol$col <- rgb(.2, .4, .2, .5)
theme1$plot.symbol$pch <- 16
theme1$plot.line$col <- rgb(.8, .1, .1, 1)
theme1$plot.line$lwd <- 2
theme1$strip.background$col <- rgb(.0, .2, .6, .2)
trellis.par.set(theme1)

featurePlot(x = dat_train1[ ,1:11],
            y = dat_train1[ ,13],
            plot = "scatter",
            span = .5,
            labels = c("Predictors","Log Recovery Time"),
            type = c("p", "smooth"))

```
Fit LASSO
```{r}
ctrl1 <- trainControl(method = "repeatedcv", number = 10, repeats = 5)

set.seed(5220)
lasso.fit <- train(x_train, y_train,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1,
                                          lambda = exp(seq(5, -1, length=100))),
                   trControl = ctrl1)
plot(lasso.fit, xTrans = log)

lasso.fit$bestTune
coef(lasso.fit$finalModel, lasso.fit$bestTune$lambda)
```


Fit Ridge
```{r}
set.seed(5220)
ridge.fit <- train(x_train, y_train,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 0,
                   lambda = exp(seq(10, -2, length=100))),
                   trControl = ctrl1)
plot(ridge.fit, xTrans = log)

ridge.fit$bestTune
coef(ridge.fit$finalModel, ridge.fit$bestTune$lambda)
```


Fit Elastic net
```{r}
set.seed(5220)
enet.fit <- train(x_train, y_train,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 21),
                  lambda = exp(seq(2, -2, length = 50))),
                  trControl = ctrl1)
enet.fit$bestTune

coef(enet.fit$finalModel, enet.fit$bestTune$lambda)
```


Fit PCR
```{r}
ctrl2 <- trainControl(method = "repeatedcv",
                      number = 10,
                      repeats = 5,
                      selectionFunction = "best")

set.seed(5220)
pcr.fit <- train(x_train, y_train,
                 method = "pcr",
                 tuneGrid = data.frame(ncomp = 1:13),
                 trControl = ctrl2,
                 preProcess = c("center", "scale"))

ggplot(pcr.fit, highlight = TRUE) + theme_bw()
summary(pcr.fit)
```


Fit PLS
```{r}
set.seed(5220)
pls.fit <- train(x_train, y_train,
                 method = "pls",
                 tuneGrid = data.frame(ncomp = 1:13),
                 trControl = ctrl2,
                 preProcess = c("center", "scale"))

ggplot(pls.fit, highlight = TRUE)

summary(pls.fit)
```


Fit GAM
```{r}
#ctrl3 <- trainControl(method = "cv", number = 10)

set.seed(5220)
gam.fit <- train(x_train, y_train,
                 method = "gam",
                 trControl = ctrl1)
gam.fit$bestTune

gam.fit$finalModel

coef(gam.fit$finalModel)


mod_gam <- gam(log_recovery_time ~ gender + race + smoking + hypertension + 
    diabetes + vaccine + severity + s(age) + 
    s(sbp) + s(ldl) + s(bmi),
               data = dat[trainRows,], method = "REML")

summary(mod_gam)


par(mfrow = c(1,6)) 
plot(gam.fit$finalModel)
```


Fit MARS
```{r}
mars_grid <- expand.grid(degree = 1:3,
                         nprune = 2:15)
set.seed(5220)
mars.fit <- train(x_train, y_train,
                  method = "earth",
                  tuneGrid = mars_grid,
                  trControl = ctrl1)
ggplot(mars.fit)

mars.fit$bestTune

coef(mars.fit$finalModel)
```



Model Comparison
```{r}
set.seed(5220)
resamp <- resamples(list(enet = enet.fit, lasso = lasso.fit, ridge = ridge.fit, pcr = pcr.fit, pls = pls.fit, gam=gam.fit))
summary(resamp)

bwplot(resamp, metric = "RMSE")
```

Regression tree
```{r}
set.seed(5220)

rpart.fit <- train(recovery_time ~ . ,
                   data = dat_train,
                   method = "rpart", 
                   tuneGrid = data.frame(cp = exp(seq(-6,-2, length = 50))),
                   trControl = ctrl1)
rpart.fit$bestTune
# Plot of the complexity parameter
ggplot(rpart.fit, highlight = TRUE) 

# Variable importance
plot(varImp(rpart.fit, scale = TRUE))

rpart.plot(rpart.fit$finalModel)

pred.rf <- predict(rpart.fit, newdata = dat_test)
RMSE(pred.rf, dat_test$recovery_time)
```

Random forest
```{r}
rf.grid <- expand.grid(mtry = 1:11,
                       splitrule = "variance",
                       min.node.size = 1:6)

set.seed(5220)

rf.fit <- train(recovery_time ~ .,
               data = dat_train,
                method = "ranger",
                tuneGrid = rf.grid,
                trControl = ctrl1)

ggplot(rf.fit, highlight = TRUE)
rf.fit$bestTune
```



```{r, warning=FALSE}
# variable importance using permutation methods 
set.seed(5220)
rf.perm = ranger(recovery_time ~ ., 
                      data = dat_train,
                      mtry = rf.fit$bestTune[[1]],
                      splitrule = "variance",
                      min.node.size = rf.fit$bestTune[[3]],
                      importance = "permutation",
                      scale.permutation.importance = TRUE)

barplot(sort(ranger::importance(rf.perm), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7, 
        col = colorRampPalette(colors = c("cyan", "blue"))(19))

# variable importance using impurity methods
set.seed(5220)
rf.impu <- ranger(recovery_time ~ .,
                        data = dat_train,
                        mtry = rf.fit$bestTune[[1]],
                        splitrule = "variance",
                        min.node.size = rf.fit$bestTune[[3]],
                        importance = "impurity")

barplot(sort(ranger::importance(rf.impu), decreasing = FALSE),
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("cyan", "blue"))(19))
```


```{r, warning=FALSE}
# test error
pred.rf <- predict(rf.fit, newdata = dat_test)
RMSE(pred.rf, dat_test$recovery_time)
```

Boosting
```{r, warning=FALSE}
bst.grid = expand.grid(n.trees = c(2000,3000,4000,5000),
                        interaction.depth = 1:3,
                        shrinkage = c(0.005,0.01),
                        n.minobsinnode = c(1))

set.seed(5220)
bst.fit <- train(recovery_time ~.,
                 data = dat_train,
                 method = "gbm",
                 tuneGrid = bst.grid,
                 trControl = ctrl1,
                 verbose = FALSE)

ggplot(bst.fit, highlight = TRUE)
bst.fit$bestTune
```


```{r, warning=FALSE}
# variable importance
summary(bst.fit$finalModel, las = 2, cBars = 11, cex.names = 0.6)

# test error
pred.bst <- predict(bst.fit, newdata = dat_test)
RMSE(pred.bst, dat_test$recovery_time)

```