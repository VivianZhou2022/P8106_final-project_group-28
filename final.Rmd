---
title: "final_recovery_time"
output:
  pdf_document: default
  html_document: default
date: "2023-05-10"
---

```{r setup, include=FALSE}
library(ggplot2)
library(glmnet)
library(MASS)
library(tidyverse)
library(cowplot)
library(corrgram)
library(dplyr)
library(caret)
library(reshape2)
library(plyr)
library(corrplot)
library(rpart)
library(rpart.plot)
library(readr)
library(ranger)
```

Data Cleaning
```{r}
dat0 = read_csv(file = "final_used_data.csv")
dat1 <- na.omit(dat0)

#Create a new data frame without id variable
dat <- dat1[ , !names(dat0) %in% c("id")]
attach(dat)

```

Data Preprocessing
```{r}
#Visualize response variable
par(mfrow = c(1, 2))
boxplot(dat$recovery_time, main = "COVID-19 Recovery Time")
hist(dat$recovery_time, main = "Distribution of Rescovery Time", col = "lightblue",
                        xlab = "Recovery Time", prob = TRUE, ylim = c(0,0.03))
lines(density(dat$recovery_time))
abline(v = mean(dat$recovery_time), lty = "dashed", col = "red")
# The above plots show that the response variable is right-skewed, so log-transformation was performed. 


dat$log_recovery_time <- log(dat$recovery_time)
par(mfrow = c(1, 2))
boxplot(dat$log_recovery_time, main = "COVID-19 Recovery Time")
hist(dat$log_recovery_time, main = "Distribution of Rescovery Time", col = "lightblue",
xlab = "Log Recovery Time", prob = TRUE, ylim = c(0,1))
lines(density(dat$log_recovery_time))
abline(v = mean(dat$log_recovery_time), lty = "dashed", col = "red")
#After the transformation, the outcome variable is normally distributed.
```

Data Partition
```{r split}
set.seed(5220)
trainRows <- createDataPartition(y = dat$log_recovery_time, p = 0.8, list = FALSE)

# Training data
dat_train = dat[trainRows, ]
x_train = model.matrix(log_recovery_time~.,dat)[trainRows, -1]
y_train = dat$log_recovery_time[trainRows]
# Test data
dat_test = dat[-trainRows, ]
x_test = model.matrix(log_recovery_time~.,dat)[-trainRows, -1]
y_test = dat$log_recovery_time[-trainRows]

```

Exploratory Analysis & Data Visualization
```{r}
# Summary statistics for each variable
summary(dat_train)

#Relocate columns putting non-discrete predictors together
dat1 =
  dat %>% 
  select(age,bmi,sbp,ldl,recovery_time)


# Correlation Plot
dat2 <- model.matrix(log_recovery_time ~ ., dat1)[ ,-1]
x <- dat2[trainRows,]
corrplot(cor(x))


# Convert non-numeric columns to numeric
dat_train1 <- dat_train
non_numeric_cols <- sapply(dat_train1, function(x) !is.numeric(x))
dat_train1[, non_numeric_cols] <- lapply(dat_train1[, non_numeric_cols], as.numeric)


```
Fit LASSO
```{r}
ctrl1 <- trainControl(method = "repeatedcv", number = 10, repeats = 5)

set.seed(5220)
lasso.fit <- train(x_train, y_train,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1,
                                          lambda = exp(seq(5, -1, length=100))),
                   trControl = ctrl1)
plot(lasso.fit, xTrans = log)

lasso.fit$bestTune
coef(lasso.fit$finalModel, lasso.fit$bestTune$lambda)

```


Fit Elastic net
```{r}
set.seed(5220)
enet.fit <- train(x_train, y_train,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 21),
                  lambda = exp(seq(2, -2, length = 50))),
                  trControl = ctrl1)
enet.fit$bestTune

coef(enet.fit$finalModel, enet.fit$bestTune$lambda)

plot(enet.fit)
```


Fit PCR
```{r}
ctrl2 <- trainControl(method = "repeatedcv",
                      number = 10,
                      repeats = 5,
                      selectionFunction = "best")

set.seed(5220)
pcr.fit <- train(x_train, y_train,
                 method = "pcr",
                 tuneGrid = data.frame(ncomp = 1:13),
                 trControl = ctrl2,
                 preProcess = c("center", "scale"))

ggplot(pcr.fit, highlight = TRUE) + theme_bw()
summary(pcr.fit)
```


Fit PLS
```{r}
set.seed(5220)
pls.fit <- train(x_train, y_train,
                 method = "pls",
                 tuneGrid = data.frame(ncomp = 1:13),
                 trControl = ctrl2,
                 preProcess = c("center", "scale"))

ggplot(pls.fit, highlight = TRUE)

summary(pls.fit)
```


Fit GAM
```{r}
#ctrl3 <- trainControl(method = "cv", number = 10)

set.seed(5220)
gam.fit <- train(x_train, y_train,
                 method = "gam",
                 trControl = ctrl1)
gam.fit$bestTune

gam.fit$finalModel

coef(gam.fit$finalModel)


mod_gam <- gam(log_recovery_time ~ gender + race + smoking + hypertension + 
    diabetes + vaccine + severity + s(age) + 
    s(sbp) + s(ldl) + s(bmi),
               data = dat[trainRows,], method = "REML")

summary(mod_gam)


par(mfrow = c(1,6)) 
plot(gam.fit$finalModel)


```


Fit MARS
```{r}
mars_grid <- expand.grid(degree = 1:3,
                         nprune = 2:15)
set.seed(5220)
mars.fit <- train(x_train, y_train,
                  method = "earth",
                  tuneGrid = mars_grid,
                  trControl = ctrl1)
ggplot(mars.fit)

mars.fit$bestTune

coef(mars.fit$finalModel)
```



Model Comparison
```{r}
set.seed(5220)
resamp <- resamples(list(enet = enet.fit, lasso = lasso.fit, pcr = pcr.fit, pls = pls.fit, gam=gam.fit))
summary(resamp)

bwplot(resamp, metric = "RMSE")
```

Regression tree
```{r}
set.seed(5220)

rpart.fit <- train(recovery_time ~ . ,
                   data = dat_train,
                   method = "rpart", 
                   tuneGrid = data.frame(cp = exp(seq(-6,-2, length = 50))),
                   trControl = ctrl1)
rpart.fit$bestTune
# Plot of the complexity parameter
ggplot(rpart.fit, highlight = TRUE) 

# Variable importance
plot(varImp(rpart.fit, scale = TRUE))

rpart.plot(rpart.fit$finalModel)

pred.rf <- predict(rpart.fit, newdata = dat_test)
RMSE(pred.rf, dat_test$recovery_time)
```

Random forest
```{r}
rf.grid <- expand.grid(mtry = 1:11,
                       splitrule = "variance",
                       min.node.size = 1:6)

set.seed(5220)

rf.fit <- train(recovery_time ~ .,
               data = dat_train,
                method = "ranger",
                tuneGrid = rf.grid,
                trControl = ctrl1)

ggplot(rf.fit, highlight = TRUE)
rf.fit$bestTune
```



```{r, warning=FALSE}
# variable importance using permutation methods 
set.seed(5220)
rf.perm = ranger(recovery_time ~ ., 
                      data = dat_train,
                      mtry = rf.fit$bestTune[[1]],
                      splitrule = "variance",
                      min.node.size = rf.fit$bestTune[[3]],
                      importance = "permutation",
                      scale.permutation.importance = TRUE)

barplot(sort(ranger::importance(rf.perm), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7, 
        col = colorRampPalette(colors = c("cyan", "blue"))(19))

# variable importance using impurity methods
set.seed(5220)
rf.impu <- ranger(recovery_time ~ .,
                        data = dat_train,
                        mtry = rf.fit$bestTune[[1]],
                        splitrule = "variance",
                        min.node.size = rf.fit$bestTune[[3]],
                        importance = "impurity")

barplot(sort(ranger::importance(rf.impu), decreasing = FALSE),
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("cyan", "blue"))(19))
```


```{r, warning=FALSE}
# test error
pred.rf <- predict(rf.fit, newdata = dat_test)
RMSE(pred.rf, dat_test$recovery_time)
```

Boosting
```{r, warning=FALSE}
bst.grid = expand.grid(n.trees = c(2000,3000,4000,5000),
                        interaction.depth = 1:3,
                        shrinkage = c(0.005,0.01),
                        n.minobsinnode = c(1))

set.seed(5220)
bst.fit <- train(recovery_time ~.,
                 data = dat_train,
                 method = "gbm",
                 tuneGrid = bst.grid,
                 trControl = ctrl1,
                 verbose = FALSE)

ggplot(bst.fit, highlight = TRUE)
bst.fit$bestTune
```


```{r, warning=FALSE}
# variable importance
summary(bst.fit$finalModel, las = 2, cBars = 11, cex.names = 0.6)

# test error
pred.bst <- predict(bst.fit, newdata = dat_test)
RMSE(pred.bst, dat_test$recovery_time)

```

Model Comparison
```{r}
set.seed(5220)
resamp <- resamples(list(enet = enet.fit, lasso = lasso.fit, pcr = pcr.fit, pls = pls.fit, gam=gam.fit, tree=rpart.fit, rf=rf.fit, boosting=bst.fit))
summary(resamp)

bwplot(resamp, metric = "RMSE")
```

secondar analysis


```{r}

library(summarytools)
library(glmnet)
library(caret)
library(corrplot)

library(ISLR)
library(plotmo)

library(caret)
library(glmnet)
library(mlbench)
library(pROC)
library(pdp)
library(rpart)
library(rpart.plot)
library(gbm)
library(tree)
library(caret)
library(party)
library(ranger)
library(randomForest)

library(e1071)
```
1. Impotant data and treat recovery time as binary
```{r setup, include=FALSE}

data = read.csv("final_used_data.csv")
#qing_zhou_data = read.csv("Qing Zhou_data_id.csv")
#data <- subset(data, select = -c(height, weight, study))
#names(data) <- tolower(names(data))
#write.csv(unique_merged_data, "final_used_data.csv", row.names = FALSE)
#merged_data <- rbind(data, qing_zhou_data)
#unique_merged_data <- unique(merged_data)
data$binary_recovery <- ifelse(data$recovery_time > 30, 1, 0)
table(data$binary_recovery)
```
2. produce the training set
```{r cars}
set.seed(2023)
rowTrain <- createDataPartition(y = data$recovery_time,
p = 0.8,
list = FALSE)
data$binary_recovery <- factor(data$binary_recovery)
contrasts(data$binary_recovery)
dat = data[, -1]
dat$binary_recovery <- factor(dat$binary_recovery)
```

3.condcut regression
Therer are no significant p value, indicating the logistic regression is not a suitable model for the secondary analysis
```{r}
glm.fit <- glm(binary_recovery ~ .,
              data = dat[rowTrain, ],
              subset = rowTrain,
              family = binomial(link = "logit"))
summary(glm.fit)
```
4. roc curve
```{r}
test.pred.prob <- predict(glm.fit, newdata = dat[-rowTrain,],
                type = "response")
                test.pred <- rep("0", length(test.pred.prob))
                test.pred[test.pred.prob>0.5] <- "1"
                confusionMatrix(data = as.factor(test.pred),
                reference = dat$binary_recovery[-rowTrain],
                positive = "1")
test.pred <- ifelse(test.pred.prob > 0.5, 1, 0)
confusionMatrix(data = as.factor(test.pred),
                reference = dat$binary_recovery[-rowTrain],
                positive = "1")
roc.glm <- roc(dat$binary_recovery[-rowTrain], test.pred.prob)
plot(roc.glm, legacy.axes = TRUE, print.auc = TRUE)
#plot(smooth(roc.glm), col = 4, add = TRUE)


```


5. classification tree
rpart
```{r}
tree1 <- rpart(formula = binary_recovery ~ . ,
data = dat,
subset = rowTrain,
control = rpart.control(cp = 0))
cpTable <- printcp(tree1)
plotcp(tree1)
minErr <- which.min(cpTable[,4])
tree2 <- prune(tree1, cp = cpTable[minErr,1])
rpart.plot(tree2)
summary(tree2)

#cross validation
folds <- createFolds(dat$binary_recovery, k = 10)
ctrl <- trainControl(method = "cv", 
                     index = folds,
                     savePredictions = TRUE,
                     classProbs = TRUE)

levels(dat$binary_recovery) <- make.names(levels(dat$binary_recovery))
model <- train(binary_recovery ~ ., 
               data = dat, 
               method = "rpart", 
               trControl = ctrl)

ggplot(model, highlight = TRUE) 
print(model)

rpart.fit <- train(recovery_time ~ . ,
                   data = dat,
                   method = "rpart", 
                   tuneGrid = data.frame(cp = exp(seq(-6,-2, length = 50))),
                   trControl = ctrl1)
rpart.fit$bestTune
rpart.plot(rpart.fit$finalModel)
```
Based on the output, the classification tree model was trained using cross-validation on a dataset with 2000 samples and 15 predictors. The model was evaluated using accuracy and kappa metrics and optimized using the largest accuracy value. The optimal model had a complexity parameter (cp) value of 0.5, which resulted in an accuracy of 0.997 and kappa of 0.994. This suggests that the model performs very well in predicting the binary_recovery outcome variable, and the chosen cp value effectively balances the bias-variance tradeoff in the model.


6. random forest

```{r}
dat = data[, -1]
set.seed(1)
bagging <- randomForest(binary_recovery ~ . ,
dat[rowTrain,],
mtry = 8)
set.seed(1)
rf <- randomForest(binary_recovery ~ . ,
                    dat[rowTrain,],
                    mtry = 3)
                    set.seed(1)
                    rf2 <- ranger(binary_recovery ~ . ,
                    dat[rowTrain,],
                    mtry = 3,
                    probability = TRUE)

rf.pred <- predict(rf, dat[-rowTrain,])
confusionMatrix(data=rf.pred, reference=dat[-rowTrain,]$binary_recovery, positive="1")

#summary(dat)
rf.grid <- expand.grid(mtry = 1:11,
splitrule = "variance",
 min.node.size = 1:6)
set.seed(2266)
ctrl1 <- trainControl(method = "cv")
# train a random forest model on the training data
rf.fit <- train(recovery_time ~ .,
               data = dat,
                method = "ranger",
                tuneGrid = rf.grid,
                trControl = ctrl1)
ggplot(rf.fit, highlight = TRUE)
```
The random forest model performed very well, with an accuracy of 97.99% and a kappa of 0.9514, indicating very good agreement between predicted and actual values. The confusion matrix shows that the model correctly classified 391 out of 399 observations. The sensitivity and specificity of the model were both very high (93.33% and 100% respectively), and the positive predictive value and negative predictive value were both very good (100% and 97.21% respectively). Overall, the model appears to be very accurate and reliable.

7. SVM
```{r}
set.seed(1)
linear.tune <- tune.svm(binary_recovery ~ . ,
                data = dat[rowTrain,],
                kernel = "linear",
                cost = exp(seq(-5,2,len=50)),
                scale = TRUE)
plot(linear.tune)
linear.tune$best.parameters
best.linear <- linear.tune$best.model
summary(best.linear)

pred.linear <- predict(best.linear, newdata = dat[-rowTrain,])
confusionMatrix(data = pred.linear,
                reference = dat$binary_recovery[-rowTrain])



```

The SVM model achieved an accuracy of 0.9799, with a Kappa value of 0.9514. The model correctly classified 112 out of 120 non-recovery instances and 279 out of 279 recovery instances. The model performed better than the baseline No Information Rate of 0.6992, indicating that it was able to effectively separate the two classes.


```{r}
library(ggplot2)

# Parameter Tuning Plot
linear_tune <- tune.svm(binary_recovery ~ .,
                        data = dat[rowTrain, ],
                        kernel = "linear",
                        cost = exp(seq(-5, 2, len = 50)),
                        scale = TRUE)
linear_tune$results$cost <- exp(seq(-5, 2, len = 50))
linear_tune_df <- do.call(rbind, linear_tune$results)  # Convert list to data frame

linear_tune_plot <- ggplot(linear_tune_df, aes(x = log(cost), y = Accuracy)) +
  geom_line() +
  labs(x = "log(Cost)", y = "Accuracy") +
  ggtitle("Linear SVM Parameter Tuning") +
  theme_minimal()

print(linear_tune_plot)


# Variable Importance Plot
bagging <- randomForest(binary_recovery ~ .,
                       data = dat[rowTrain, ],
                       mtry = 8)

bagging_importance <- importance(bagging)
bagging_importance_plot <- ggplot(bagging_importance, aes(x = Importance, y = reorder(Variable, Importance))) +
  geom_point(size = 3) +
  labs(x = "Variable Importance", y = "Variable") +
  ggtitle("Random Forest (Bagging) Variable Importance") +
  theme_minimal()

print(bagging_importance_plot)



```
